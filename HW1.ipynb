{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "6526ce64-e498-4f99-bfb3-c20758bc5c66",
      "cell_type": "code",
      "source": "# Задание 1: Реализация Gradient Descent для модели линейной регрессии\n\nimport numpy as np\n\nclass LinearRegressorGD:\n\n    def __init__(self, learning_rate=0.01, n_iter=1000):\n        self.learning_rate = learning_rate\n        self.n_iter = n_iter\n\n    def fit(self, X, y):\n        self.W = np.zeros(X.shape[1])\n        self.b = 0.0\n        for i in range(self.n_iter):\n            y_pred = self.predict(X)\n            err = y_pred - y\n            self.W -= self.learning_rate * 2 * np.dot(X.T, err)/ X.shape[0] #вот этот момент где ноль еще уточниить\n            self.b -= self.learning_rate * np.sum(err) / X.shape[0]\n\n    def predict(self, X):\n        return np.dot(X, self.W) + self.b\n\n    def get_params(self):\n        return self.W, self.b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "41aa7746-3780-4a25-8f6a-5e97cfff42c7",
      "cell_type": "code",
      "source": "# Задание 2: Реализация backpropagation для MLP\n\nclass MLPRegressor:\n   \n    def __init__(self, hidden_layer_sizes=(100,), learning_rate=0.01, n_iter=100):\n        self.hidden_layer_sizes = hidden_layer_sizes\n        self.learning_rate = learning_rate\n\n    def sigmoid(self, x, deriv=False):\n        if deriv == True:\n            z = sigmoid(x)\n            return z * (1-z)\n        return 1/(1 + np.exp(-x))\n        \n    def forward(self, X):\n        self.z = np.dot(self.W, X)\n        self.a = self.sigmoid(self.z)\n        self.z2 = np.dot(self.a, self.W2)\n        output = self.z2\n        return output\n\n    def backward(self, X, y):\n        self.delta_error = 2 * (y - output) / X.shape[0]\n        delta_z = self.delta_error #тк как функция активации последнего слоя линейная\n        z_error = np.dot(delta_z, W2.T)\n        delta_z = self.z_error * self.sigmoid(self.z_error, True)\n        #обновление весов\n\n    def fit(self, X, y):\n        #инициализация весов мб\n        for i in range(self.n_iter):\n            self.forward(X)\n            self.backward(X, y)\n\n    def predict(self, X):\n        return self.forward(X)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d421fd17-755d-4723-bc08-9135c6dc7980",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
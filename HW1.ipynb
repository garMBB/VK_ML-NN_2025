{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "6526ce64-e498-4f99-bfb3-c20758bc5c66",
      "cell_type": "code",
      "source": "# Задание 1: Реализация Gradient Descent для модели линейной регрессии\n\nimport numpy as np\n\nclass LinearRegressorGD:\n\n    def __init__(self, learning_rate=0.01, n_iter=1000):\n        self.learning_rate = learning_rate\n        self.n_iter = n_iter\n\n    def fit(self, X, y):\n        self.W = np.zeros(X.shape[1])\n        self.b = 0.0\n        for i in range(self.n_iter):\n            y_pred = self.predict(X)\n            err = y_pred - y\n            self.W -= self.learning_rate * 2 * np.dot(X.T, err)/ X.shape[0]\n            self.b -= self.learning_rate * 2 * np.sum(err) / X.shape[0]\n\n    def predict(self, X):\n        return np.dot(X, self.W) + self.b\n\n    def get_params(self):\n        return self.W, self.b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "41aa7746-3780-4a25-8f6a-5e97cfff42c7",
      "cell_type": "code",
      "source": "# Задание 2: Реализация backpropagation для MLP для одного скрытого слоя \n\nclass MLPRegressor:\n   \n    def __init__(self, hidden_layer_sizes=(100,), learning_rate=0.01, n_iter=100):\n        self.hidden_layer_sizes = hidden_layer_sizes\n        self.learning_rate = learning_rate\n\n    def sigmoid(self, x, deriv=False):\n        f = 1 / (1 + np.exp(-x))\n        if deriv:\n            return f * (1 - f)\n        return f\n        \n    def forward(self, X):\n        self.z1 = np.dot(X, self.w1) + self.b1\n        self.a = self.sigmoid(self.z1)\n        self.z2 = np.dot(self.a, self.w2) + self.b2\n        self.output = self.z2\n        return self.output\n\n    def backward(self, X, y):\n        delta_z2 = 2 * (self.output - y) / X.shape[0]\n        delta_w2 = np.dot(self.a.T, delta_z2) #тк как функция активации последнего слоя линейная\n        \n        delta_a = np.dot(delta_z2, self.w2.T)\n        delta_z1 = delta_a * self.sigmoid(self.z1, deriv=True)\n        delta_w1 = np.dot(X.T, delta_z1)\n\n        self.w1 -= self.learning_rate * delta_w1\n        self.b1 -= self.learning_rate * np.sum(delta_z1, axis=0)\n        self.w2 -= self.learning_rate * delta_w2\n        self.b2 -= self.learning_rate * np.sum(delta_z2, axis=0)\n        \n    def fit(self, X, y):\n        self.w1 = np.random.randn(X.shape[1], self.hidden_layer_sizes) * 0.01\n        self.w2 = np.random.randn(self.hidden_layer_sizes, 1) * 0.01\n        self.b1 = np.zeros((1, self.hidden_layer_sizes))\n        self.b2 = np.zeros((1,1))\n\n        y = y.reshape(-1, 1)\n        for i in range(self.n_iter):\n            self.forward(X)\n            self.backward(X, y)\n\n    def predict(self, X):\n        return self.forward(X)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
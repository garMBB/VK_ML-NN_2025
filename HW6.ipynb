{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8548b394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INadtochii\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "from torchvision.ops import box_iou\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18637a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to 'C:\\Users\\INadtochii\\fiftyone\\open-images-v7\\train' if necessary\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/train/train-images-boxable-with-rotation.csv' to 'C:\\Users\\INadtochii\\fiftyone\\open-images-v7\\train\\metadata\\image_ids.csv'\n",
      " 100% |██████|    4.8Gb/4.8Gb [52.9s elapsed, 0s remaining, 102.4Mb/s]     \n",
      "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to 'C:\\Users\\INadtochii\\fiftyone\\open-images-v7\\train\\metadata\\classes.csv'\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to 'C:\\Users\\INADTO~1\\AppData\\Local\\Temp\\tmpv36lzvw1\\metadata\\hierarchy.json'\n",
      "Downloading 'https://storage.googleapis.com/openimages/v6/oidv6-train-annotations-bbox.csv' to 'C:\\Users\\INadtochii\\fiftyone\\open-images-v7\\train\\labels\\detections.csv'\n",
      "Downloading 400 images\n",
      " 100% |███████████████████| 400/400 [22.1s elapsed, 0s remaining, 5.0 files/s]       \n",
      "Dataset info written to 'C:\\Users\\INadtochii\\fiftyone\\open-images-v7\\info.json'\n",
      "Loading 'open-images-v7' split 'train'\n",
      " 100% |█████████████████| 400/400 [1.7s elapsed, 0s remaining, 236.8 samples/s]         \n",
      "Dataset 'open-images-v7-train-400' created\n",
      "Downloading split 'validation' to 'C:\\Users\\INadtochii\\fiftyone\\open-images-v7\\validation' if necessary\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/validation/validation-images-with-rotation.csv' to 'C:\\Users\\INadtochii\\fiftyone\\open-images-v7\\validation\\metadata\\image_ids.csv'\n",
      "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to 'C:\\Users\\INadtochii\\fiftyone\\open-images-v7\\validation\\metadata\\classes.csv'\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to 'C:\\Users\\INADTO~1\\AppData\\Local\\Temp\\tmps6jxo9cx\\metadata\\hierarchy.json'\n",
      "Downloading 'https://storage.googleapis.com/openimages/v5/validation-annotations-bbox.csv' to 'C:\\Users\\INadtochii\\fiftyone\\open-images-v7\\validation\\labels\\detections.csv'\n",
      "Downloading 50 images\n",
      " 100% |█████████████████████| 50/50 [5.1s elapsed, 0s remaining, 10.0 files/s]      \n",
      "Dataset info written to 'C:\\Users\\INadtochii\\fiftyone\\open-images-v7\\info.json'\n",
      "Loading 'open-images-v7' split 'validation'\n",
      " 100% |███████████████████| 50/50 [1.2s elapsed, 0s remaining, 41.6 samples/s]          \n",
      "Dataset 'open-images-v7-validation-50' created\n",
      "Downloading split 'test' to 'C:\\Users\\INadtochii\\fiftyone\\open-images-v7\\test' if necessary\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/test/test-images-with-rotation.csv' to 'C:\\Users\\INadtochii\\fiftyone\\open-images-v7\\test\\metadata\\image_ids.csv'\n",
      "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to 'C:\\Users\\INadtochii\\fiftyone\\open-images-v7\\test\\metadata\\classes.csv'\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to 'C:\\Users\\INADTO~1\\AppData\\Local\\Temp\\tmppvezkk91\\metadata\\hierarchy.json'\n",
      "Downloading 'https://storage.googleapis.com/openimages/v5/test-annotations-bbox.csv' to 'C:\\Users\\INadtochii\\fiftyone\\open-images-v7\\test\\labels\\detections.csv'\n",
      "Downloading 50 images\n",
      " 100% |█████████████████████| 50/50 [4.7s elapsed, 0s remaining, 15.6 files/s]      \n",
      "Dataset info written to 'C:\\Users\\INadtochii\\fiftyone\\open-images-v7\\info.json'\n",
      "Loading 'open-images-v7' split 'test'\n",
      " 100% |███████████████████| 50/50 [521.0ms elapsed, 0s remaining, 96.0 samples/s]      \n",
      "Dataset 'open-images-v7-test-50' created\n"
     ]
    }
   ],
   "source": [
    "train_dataset = foz.load_zoo_dataset(\n",
    "              \"open-images-v7\",\n",
    "              split=\"train\",\n",
    "              label_types=[\"detections\"],\n",
    "              classes=[\"Ball\"],\n",
    "              max_samples=400,\n",
    "          )\n",
    "\n",
    "val_dataset = foz.load_zoo_dataset(\n",
    "              \"open-images-v7\",\n",
    "              split=\"validation\",\n",
    "              label_types=[\"detections\"],\n",
    "              classes=[\"Ball\"],\n",
    "              max_samples=50,\n",
    "          )\n",
    "\n",
    "test_dataset = foz.load_zoo_dataset(\n",
    "              \"open-images-v7\",\n",
    "              split=\"test\",\n",
    "              label_types=[\"detections\"],\n",
    "              classes=[\"Ball\"],\n",
    "              max_samples=50,\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31436518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 400/400 [2.4s elapsed, 0s remaining, 175.4 samples/s]      \n",
      " 100% |███████████████████| 50/50 [411.9ms elapsed, 0s remaining, 121.4 samples/s]      \n",
      " 100% |███████████████████| 50/50 [446.1ms elapsed, 0s remaining, 112.1 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "# Экспорт в COCO\n",
    "export_dir_train = \"openimages_fiftyone_train\"\n",
    "train_dataset.export(\n",
    "    export_dir=export_dir_train,\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    label_field=\"ground_truth\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "export_dir_val = \"openimages_fiftyone_val\"\n",
    "val_dataset.export(\n",
    "    export_dir=export_dir_val,\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    label_field=\"ground_truth\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "export_dir_test = \"openimages_fiftyone_test\"\n",
    "test_dataset.export(\n",
    "    export_dir=export_dir_test,\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    label_field=\"ground_truth\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08d59cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODetectionDataset(Dataset):\n",
    "    def __init__(self, root, annFile, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Загружаем аннотации COCO\n",
    "        with open(annFile, \"r\") as f:\n",
    "            coco_data = json.load(f)\n",
    "\n",
    "        # Сопоставление id → информация об изображении\n",
    "        self.images = {img[\"id\"]: img for img in coco_data[\"images\"]}\n",
    "\n",
    "        # Группируем аннотации по image_id\n",
    "        self.annotations = {}\n",
    "        for ann in coco_data[\"annotations\"]:\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in self.annotations:\n",
    "                self.annotations[img_id] = []\n",
    "            self.annotations[img_id].append(ann)\n",
    "\n",
    "        # Список image_id\n",
    "        self.ids = list(self.images.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img_info = self.images[img_id]\n",
    "        annots = self.annotations.get(img_id, [])\n",
    "\n",
    "        img_path = os.path.join(self.root, img_info[\"file_name\"])\n",
    "        # Albumentations работает с numpy\n",
    "        img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for ann in annots:\n",
    "            xmin, ymin, w, h = ann[\"bbox\"]\n",
    "            xmax = xmin + w\n",
    "            ymax = ymin + h\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(ann[\"category_id\"])\n",
    "\n",
    "        # Если заданы трансформации (Albumentations)\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(\n",
    "                image=img,\n",
    "                bboxes=boxes,\n",
    "                class_labels=labels\n",
    "            )\n",
    "            img = transformed[\"image\"]\n",
    "            boxes = transformed[\"bboxes\"]\n",
    "            labels = transformed[\"class_labels\"]\n",
    "\n",
    "        # Преобразуем в torch тензоры\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        image_id = torch.tensor([img_id])\n",
    "\n",
    "        # Вычисляем площади и флаги\n",
    "        if boxes.numel() > 0:\n",
    "            areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "        else:\n",
    "            areas = torch.zeros((0,), dtype=torch.float32)\n",
    "\n",
    "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": areas,\n",
    "            \"iscrowd\": iscrowd\n",
    "        }\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42c269c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INadtochii\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "C:\\Users\\INadtochii\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\albumentations\\core\\composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    }
   ],
   "source": [
    "train_transform = A.Compose([\n",
    "    # Простые цветовые аугментации\n",
    "    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=0.7),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.HueSaturationValue(p=0.3),\n",
    "    A.Blur(blur_limit=3, p=0.1),\n",
    "    A.MotionBlur(blur_limit=3, p=0.1),\n",
    "    A.ToGray(p=0.05),\n",
    "    A.Resize(480, 480),\n",
    "\n",
    "    # Геометрические (затрагивают bbox)\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.ShiftScaleRotate(\n",
    "        shift_limit=0.1,\n",
    "        scale_limit=0.2,\n",
    "        rotate_limit=15,\n",
    "        border_mode=0,\n",
    "        p=0.7\n",
    "    ),\n",
    "\n",
    "    ToTensorV2()\n",
    "],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format='pascal_voc',\n",
    "        label_fields=['class_labels'],\n",
    "        min_visibility=0.3\n",
    "    )\n",
    ")\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "\n",
    "train_dataset = COCODetectionDataset(\n",
    "    root=\"openimages_fiftyone_train/data\",\n",
    "    annFile=\"openimages_fiftyone_train/labels.json\",\n",
    "    transforms=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = COCODetectionDataset(\n",
    "    root=\"openimages_fiftyone_val/data\",\n",
    "    annFile=\"openimages_fiftyone_val/labels.json\",\n",
    "    transforms=val_transform\n",
    ")\n",
    "\n",
    "test_dataset = COCODetectionDataset(\n",
    "    root=\"openimages_fiftyone_test/data\",\n",
    "    annFile=\"openimages_fiftyone_test/labels.json\",\n",
    "    transforms=None\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "subset_indices = list(range(len(train_dataset)//2))\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c390a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Train/Eval ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, optimizer, num_epochs=5):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        for images, targets in tqdm(train_loader, desc=f\"Train {epoch+1}\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += losses.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # EVAL\n",
    "        model.eval()\n",
    "        all_pred_boxes = []\n",
    "        all_gt_boxes = []\n",
    "        with torch.no_grad():\n",
    "            for images, targets in tqdm(val_loader, desc=f\"Eval {epoch+1}\"):\n",
    "                images = [img.to(device) for img in images]\n",
    "                outputs = model(images)\n",
    "\n",
    "                for out, tgt in zip(outputs, targets):\n",
    "                    pred_boxes = out[\"boxes\"].cpu()\n",
    "                    gt_boxes = tgt[\"boxes\"]\n",
    "                    all_pred_boxes.append(pred_boxes)\n",
    "                    all_gt_boxes.append(gt_boxes)\n",
    "\n",
    "        print(f\"Train loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea288eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_map(model, data_loader, coco_ann_file, device):\n",
    "    \"\"\"\n",
    "    Вычисляет COCO-style mAP для модели на заданном даталоадере\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    coco_gt = COCO(coco_ann_file)\n",
    "    coco_results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"mAP Evaluation\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for out, tgt in zip(outputs, targets):\n",
    "                boxes = out[\"boxes\"].cpu().numpy()\n",
    "                scores = out[\"scores\"].cpu().numpy()\n",
    "                labels = out[\"labels\"].cpu().numpy()\n",
    "\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    w, h = x2 - x1, y2 - y1\n",
    "                    coco_results.append({\n",
    "                        \"image_id\": int(tgt[\"image_id\"].item()),\n",
    "                        \"category_id\": int(label),\n",
    "                        \"bbox\": [float(x1), float(y1), float(w), float(h)],\n",
    "                        \"score\": float(score)\n",
    "                    })\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(coco_results)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()  # Выводит mAP@[.5:.95], mAP@0.5, mAP@0.75 и AP по размерам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da0c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "optimizer_name = \"SGD\"\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "    weights=None,\n",
    "    num_classes=2  # Ball + background\n",
    ").to(device)\n",
    "\n",
    "# Оптимизатор\n",
    "if optimizer_name == \"SGD\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "elif optimizer_name == \"Adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "elif optimizer_name == \"AdamW\":\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    raise ValueError(\"Unknown optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3dd9676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05b17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Epoch 1/5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(model, train_loader, val_loader, device, optimizer, num_epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKG8Y592X4HCsXyxi0fnoY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PdsOrbjXVDt2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "zG-mJqqdVwsc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def __init__(self, model_name: str = \"gpt2\"):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "        self.model.eval()\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def greedy_sampling(self, logits: torch.Tensor) -> int:\n",
        "        return logits.argmax(dim=-1).item()\n",
        "\n",
        "    def random_sampling(self, logits: torch.Tensor) -> int:\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        return torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "    def apply_temperature(self, logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
        "        return logits / temperature\n",
        "\n",
        "    def _apply_top_k(self, logits: torch.Tensor, top_k: int = 0) -> torch.Tensor:\n",
        "        if top_k <= 0 or top_k >= logits.size(-1):\n",
        "            return logits\n",
        "        top_k_vals, _ = torch.topk(logits, top_k)\n",
        "        min_top_k_val = top_k_vals[..., -1, None]\n",
        "        logits = torch.where(logits < min_top_k_val, torch.full_like(logits, -float('inf')), logits)\n",
        "        return logits\n",
        "\n",
        "    def _apply_top_p(self, logits: torch.Tensor, top_p: float = 1.0) -> torch.Tensor:\n",
        "        if top_p >= 1.0:\n",
        "            return logits\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = False\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(\n",
        "            dim=-1, index=sorted_indices, src=sorted_indices_to_remove\n",
        "        )\n",
        "        logits = logits.masked_fill(indices_to_remove, -float('inf'))\n",
        "        return logits\n",
        "\n",
        "    def _beam_search_generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int,\n",
        "        num_beams: int\n",
        "    ) -> str:\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n",
        "        batch_size = input_ids.shape[0]\n",
        "        beam_scores = torch.zeros((num_beams,), device=self.device)\n",
        "        beam_sequences = input_ids.expand(num_beams, -1).clone()\n",
        "        done = [False] * num_beams\n",
        "\n",
        "        cur_len = input_ids.shape[1]\n",
        "        max_len = min(max_length, self.model.config.max_position_embeddings)\n",
        "\n",
        "        while cur_len < max_len:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(beam_sequences)\n",
        "                next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            next_token_probs = F.log_softmax(next_token_logits, dim=-1)\n",
        "            next_token_scores = beam_scores.unsqueeze(-1) + next_token_probs\n",
        "\n",
        "            flat_scores = next_token_scores.view(-1)\n",
        "            top_scores, top_indices = torch.topk(flat_scores, num_beams * 2, sorted=True)\n",
        "\n",
        "            beam_indices = top_indices // self.vocab_size\n",
        "            token_indices = top_indices % self.vocab_size\n",
        "\n",
        "            next_beam_sequences = []\n",
        "            next_beam_scores = []\n",
        "            next_done = []\n",
        "            num_added = 0\n",
        "\n",
        "            for i in range(len(top_scores)):\n",
        "                beam_id = beam_indices[i].item()\n",
        "                token_id = token_indices[i].item()\n",
        "                score = top_scores[i].item()\n",
        "\n",
        "                if done[beam_id]:\n",
        "                    new_seq = beam_sequences[beam_id].clone()\n",
        "                else:\n",
        "                    new_seq = torch.cat([beam_sequences[beam_id], torch.tensor([token_id], device=self.device)], dim=0)\n",
        "\n",
        "                if token_id == self.tokenizer.eos_token_id:\n",
        "                    next_done.append(True)\n",
        "                else:\n",
        "                    next_done.append(done[beam_id])\n",
        "\n",
        "                next_beam_sequences.append(new_seq)\n",
        "                next_beam_scores.append(score)\n",
        "                num_added += 1\n",
        "                if num_added == num_beams:\n",
        "                    break\n",
        "\n",
        "            beam_sequences = torch.nn.utils.rnn.pad_sequence(\n",
        "                next_beam_sequences, batch_first=True, padding_value=self.tokenizer.eos_token_id\n",
        "            )\n",
        "            beam_scores = torch.tensor(next_beam_scores, device=self.device)\n",
        "            done = next_done\n",
        "            cur_len += 1\n",
        "\n",
        "            if all(done):\n",
        "                break\n",
        "\n",
        "        best_idx = beam_scores.argmax().item()\n",
        "        best_sequence = beam_sequences[best_idx]\n",
        "        generated_text = self.tokenizer.decode(best_sequence, skip_special_tokens=True)\n",
        "        return generated_text\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 50,\n",
        "        strategy: str = \"greedy\",\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 1.0,\n",
        "        num_beams: int = 3\n",
        "    ) -> str:\n",
        "\n",
        "        if strategy == \"beam_search\":\n",
        "            return self._beam_search_generate(prompt, max_length, num_beams)\n",
        "\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n",
        "        cur_len = input_ids.shape[1]\n",
        "        max_len = min(max_length, self.model.config.max_position_embeddings)\n",
        "\n",
        "        while cur_len < max_len:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(input_ids)\n",
        "                next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            next_token_logits = self.apply_temperature(next_token_logits, temperature)\n",
        "            next_token_logits = self._apply_top_k(next_token_logits, top_k)\n",
        "            next_token_logits = self._apply_top_p(next_token_logits, top_p)\n",
        "\n",
        "            if strategy == \"greedy\":\n",
        "                next_token = self.greedy_sampling(next_token_logits.squeeze(0))\n",
        "            elif strategy == \"random\":\n",
        "                next_token = self.random_sampling(next_token_logits.squeeze(0))\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "\n",
        "            next_token_tensor = torch.tensor([[next_token]], device=self.device)\n",
        "            input_ids = torch.cat([input_ids, next_token_tensor], dim=-1)\n",
        "            cur_len += 1\n",
        "\n",
        "            if next_token == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        return self.tokenizer.decode(input_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "t5B7QN-zVzMg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(\"gpt2\")\n",
        "\n",
        "prompt = \"Two plus five equal\"\n",
        "\n",
        "print(\"Greedy:\")\n",
        "print(model.generate(prompt, strategy=\"greedy\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDelLEDqWs-U",
        "outputId": "f6d09706-fe3f-43dc-e99f-772daeb76086"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy:\n",
            "Two plus five equal parts of the same number of points.\n",
            "\n",
            "The game is played in a round-robin format, with each player playing a single round. The first player to reach the top of the round wins. The second player to\n"
          ]
        }
      ]
    }
  ]
}